{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn1-E1LxqwYQ"
      },
      "source": [
        "# [NLP+CSS 201](https://nlp-css-201-tutorials.github.io/nlp-css-201-tutorials/): Comparing Word Embedding Models\n",
        "\n",
        "_[Sandeep Soni](http://sandeepsoni.github.io/) and [Connor Gilroy](https://ccgilroy.com/)_  \n",
        "_2021-10-13_\n",
        "\n",
        "This tutorial demonstrates the use of [`word2vec`](https://arxiv.org/abs/1301.3781) to learn vector representations of words and use them to analyze language variation and change. Specifically, it will cover:\n",
        "\n",
        "* Training a [`gensim`](https://radimrehurek.com/gensim/) model on a text corpus to learn vector representations of words and phrases.\n",
        "* Extending `word2vec` to learn vector representations of words across groups or over time, and using these representations to measure differences in word meaning.\n",
        "* Illustrating formulation and testing of social-scientific hypotheses based on vector representations of words.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5PygguBt0Uq"
      },
      "source": [
        "## I. Motivation and intuition\n",
        "\n",
        "**Why represent words as dense vectors?**\n",
        "\n",
        "Atomic units such as words are represented as dense vectors in many NLP applications. Why?\n",
        "\n",
        "* Vectors precisely capture the notion of similarity in space. If we map words to vectors, then similar vectors could correspond to similar words.\n",
        "* Vectors are easy to input as features in mathematical models.\n",
        "* Vectors are what we call as distributed representations, which are important in theory as well as in practice.\n",
        "\n",
        "To expand the last point, let's start with a toy example (modified from Dhruv Batra's slides). Assume a small vocabulary of just four words, all sport names, _{Soccer, Squash, Chess, Javelin Throw}_. So how to map these words to vectors?\n",
        "\n",
        "**1. Sparse local representation:** Represent words as exclusive sparse binary vectors.\n",
        "\n",
        "| Word          | Dim1 | Dim 2 | Dim 3 | Dim 4 |\n",
        "| ------------- | ---- | ----- | ----- | ----- |\n",
        "| Soccer        | 1    | 0     | 0     | 0     |\n",
        "| Squash        | 0    | 1     | 0     | 0     |\n",
        "| Chess         | 0    | 0     | 1     | 0     |\n",
        "| Javelin Throw | 0    | 0     | 0     | 1     |\n",
        "\n",
        "\n",
        "- Pros:\n",
        "  - Every word gets a unique representation.\n",
        "  - The mapping from words to vectors is easy.\n",
        "- Cons:\n",
        "  - Vector size expands with vocabulary size.\n",
        "  - The vectors don't capture _meaning_ -- there's no mathematical notion of more similar words being more closely related.\n",
        "\n",
        "**Dense distributed representation:** Share dimensions across words if they have the same trait.\n",
        "\n",
        "\n",
        "| Word          | Ball game | Non-square arena | Olympic Game | Indoor |\n",
        "| ------------- | --------- | ---------------- | ------------ | ------ |\n",
        "| Soccer        | 1         | 0                | 1            | 0      |\n",
        "| Javelin Throw | 0         | 1                | 1            | 0      |\n",
        "| Squash        | 1         | 0                | 0            | 1      |\n",
        "| Chess         | 0         | 1                | 0            | 1      |\n",
        "\n",
        "- Pros:\n",
        "  - Unique, interpretable representation for each word.\n",
        "  - Vectors capture meaning (with interpretable semantic dimensions, in this toy example).\n",
        "  - Number of dimensions is less than the size of the vocabulary.\n",
        "\n",
        "Rather than manually encoding representations of words, **word embedding** approaches learn dense word vectors from large text corpora instead. The dimensions of these vectors are not as easily interpretable and each dimension is real valued, but similar words have comparable values across a few dimensions.\n",
        "\n",
        "This tutorial demonstrates the use of _word2vec_, which learns dense vector representations using a shallow neural network through one of two prediction tasks based on local windows around a given word:\n",
        "\n",
        "- **skipgram**: predict the surrounding local context, given the current word\n",
        "- **CBOW**: predict the current word, given the surrounding context\n",
        "\n",
        "**What's the justification to do this?** There are many ways to learn meanings but one way is using the [distributional hypothesis](https://aclweb.org/aclwiki/Distributional_Hypothesis) which is summarized by a simple but powerfule idea that \"you many know the meaning of a word by the company it keeps\".\n",
        "\n",
        "\n",
        "In order to spend more time on practical applications, we won't cover the full mathematical background for the method here. For more detail, see the original word2vec papers ([Mikolov et al 2013a](https://arxiv.org/pdf/1301.3781.pdf) and [Mikolov et al 2013b](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)) or [Jay Alammar's visual explainer](https://jalammar.github.io/illustrated-word2vec/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-UO3Fee18qW"
      },
      "source": [
        "## II. Training a word2vec model\n",
        "\n",
        "**How are word embeddings learned in practice?**\n",
        "\n",
        "If you want to use word embeddings in your research, you have a series of options, in increasing complexity and flexibility:\n",
        "\n",
        "- using the vectors from pretrained models (e.g. [GloVe](https://nlp.stanford.edu/projects/glove/))\n",
        "- training models on your own corpus with an existing library's implementation (e.g. gensim)\n",
        "- writing your own model from scratch (e.g. in PyTorch)\n",
        "\n",
        "This section demonstrates the second option, showing how to learn embeddings with  [`gensim`](https://radimrehurek.com/gensim/). While many libraries have the functionality to learn word2vec-style word embeddings, `gensim` is robust, well-documented, and widely used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICPNv82RcszY"
      },
      "source": [
        "\n",
        "### A social science corpus: the _Congressional Record_\n",
        "\n",
        "To train a model, we need a corpus. The example corpus for this tutorial comes from the [_Congressional Record_](https://www.congress.gov/congressional-record), which records all speeches given on the floor of the US Congress. Of course, political speech is of clear substantive interest to political scientists! For the rest of us, it provides an example of relatively clean, formal text with some useful metadata.\n",
        "\n",
        "Ordinarily, you'd have to preprocess the text data for training a word2vec on your own, which can be time-consuming and which requires a number of (potentially consequential!) researcher decisions. Conveniently for us, however, the CR corpus has already been preprocessed by a number of researchers:\n",
        "\n",
        "- The corpus was originally constructed in plaintext format by Gentzkow, Shapiro, and Taddy (2018) ([repository for full download](https://data.stanford.edu/congress_text), [license](https://opendatacommons.org/licenses/by/1-0/)).\n",
        "- It was preprocessed specifically for word2vec models by Rodriguez and Spirling (2021) ([code](https://github.com/prodriguezsosa/EmbeddingsPaperReplication/blob/main/code/estimation/preprocess_cr.R), [R data file](https://www.dropbox.com/sh/jsyrag7opfo7l7i/AACzhO8d8xFJucWgApbEGggPa/data?dl=0)). To preprocess the data, Rodriguez and Spirling remove non-alphabetic characters, lowercase, and remove words of length 2 or less, then filter to Congressional sessions 111-114 (Jan 2009 - Jan 2017) and to speakers with party labels D and R.\n",
        "- For this tutorial, we converted Rodriguez and Spirling's R data file to plaintext and csv files, and we subsampled to a smaller corpus for convenience. (See the appendix at the end for the full corpus!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwj12kfTcpNN"
      },
      "source": [
        "### Setting up and training our first model\n",
        "\n",
        "In addition to `gensim`, we'll load a number of data science and utility libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhSBqHS-4bPE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import urllib.request\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "512mwxT6_sjW"
      },
      "source": [
        "Let's begin by reading the corpus from the web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJMu3wOb6ZY1"
      },
      "source": [
        "def stream_data(link_to_file, local_file):\n",
        "  \"\"\" Read the data from the web and convert into a corpus\n",
        "  that gensim can consume.\n",
        "\n",
        "  Parameters:\n",
        "  ===========\n",
        "  link_to_file (str): link to the data\n",
        "  local_file (str): local filename\n",
        "\n",
        "  Returns:\n",
        "  ========\n",
        "  corpus (LineSentence): gensim object that segments the text into sentences\n",
        "  \"\"\"\n",
        "\n",
        "  # Expected to return a list of lists or an iterable of that kind\n",
        "  # (maybe try gensim's LineSentence)\n",
        "  if not os.path.exists (local_file):\n",
        "    with urllib.request.urlopen(link_to_file) as response, open(local_file, 'wb') as out_file:\n",
        "      shutil.copyfileobj(response, out_file)\n",
        "\n",
        "  corpus = LineSentence (local_file)\n",
        "  return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRkVgcoAGtj9"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/sandeepsoni/comparing-word2vec-models/main/data/cr_subset.txt'\n",
        "local_file = 'cr_subset.txt'\n",
        "corpus = stream_data (link_to_file=url, local_file=local_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOoNb3B1b7UC"
      },
      "source": [
        "Next, let's train a word2vec model to get word embeddings and then print the most similar words to a query word. We'll use `gensim` and keep all the default settings in creating a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kWmrEo3HYKh",
        "outputId": "477e86a6-54d2-4de2-ede5-8d6eef0505de"
      },
      "source": [
        "query = 'speaker'\n",
        "model = Word2Vec (corpus, seed=42, workers=1)\n",
        "sims = model.wv.most_similar('speaker', topn=10)  # get other similar words\n",
        "for item in sims:\n",
        "  print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('chair', 0.5756440162658691)\n",
            "('chairman', 0.5422492027282715)\n",
            "('chairwoman', 0.509797990322113)\n",
            "('president', 0.49690186977386475)\n",
            "('gentlemen', 0.4256882667541504)\n",
            "('tonight', 0.3787979483604431)\n",
            "('sealevel', 0.375993013381958)\n",
            "('speakerand', 0.36592897772789)\n",
            "('representatives', 0.3384690284729004)\n",
            "('congressman', 0.3317258358001709)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUxX13yJcyLR"
      },
      "source": [
        "Because the size of the corpus is small, the model isn't necessarily the highest quality. Still, we see semantically relevant terms such as _president, chairwoman_ etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwbOMt8mCljA"
      },
      "source": [
        "### Sensitivity to parameters\n",
        "\n",
        "Recall that we trained a word2vec model using the default settings. [Here's](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) the signature of the function which contains the default settings. Let's try to see the effect of varying two parameters:\n",
        "\n",
        "* `window`: the context window size in the word2vec model.\n",
        "* `min_count`: a threshold to limit the size of the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZBJcACjEF-S"
      },
      "source": [
        "window_sizes = [2, 10, 50]\n",
        "models = list ()\n",
        "for window_size in window_sizes:\n",
        "  model = Word2Vec (corpus, window=window_size, seed=42, workers=1)\n",
        "  models.append (model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ectzn4oXDeGO",
        "outputId": "c7fba9a3-796c-4544-ed64-0004e159e998"
      },
      "source": [
        "query = 'running'\n",
        "for i in range (len (window_sizes)):\n",
        "  window_size = window_sizes[i]\n",
        "  model = models[i]\n",
        "  print (f'Model context window size={window_size}')\n",
        "  for item in model.wv.most_similar(query, topn=5):\n",
        "    print (item)\n",
        "  print ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model context window size=2\n",
            "('handing', 0.6558226943016052)\n",
            "('locked', 0.6463137865066528)\n",
            "('crying', 0.6457025408744812)\n",
            "('holding', 0.6409832239151001)\n",
            "('turned', 0.6378074884414673)\n",
            "\n",
            "Model context window size=10\n",
            "('run', 0.6504106521606445)\n",
            "('throwing', 0.6307803988456726)\n",
            "('thrown', 0.6215112209320068)\n",
            "('ran', 0.6190522313117981)\n",
            "('laying', 0.612533450126648)\n",
            "\n",
            "Model context window size=50\n",
            "('run', 0.5566155910491943)\n",
            "('roof', 0.5275007486343384)\n",
            "('busy', 0.49205806851387024)\n",
            "('ran', 0.48588094115257263)\n",
            "('sitting', 0.4697610139846802)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cevt_auCm-k"
      },
      "source": [
        "**Observations**: Increasing the window size comes at a computational expense. As the window is increased, the near neighbors move from being syntactically related (e.g. similar verbs), to semantically related (e.g. similar actions), to topically related (e.g. words that are thematically related)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGYDmlNyGnUA"
      },
      "source": [
        "min_counts = [10, 25]\n",
        "models = list ()\n",
        "for min_count in min_counts:\n",
        "  model = Word2Vec (corpus, min_count=min_count, seed=42, workers=1)\n",
        "  models.append (model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSV-kuzmG1ug",
        "outputId": "e126a2ca-4a22-4e5a-d03f-a6332b49fe11"
      },
      "source": [
        "query = 'running'\n",
        "for i in range (len (min_counts)):\n",
        "  min_count = min_counts[i]\n",
        "  model = models[i]\n",
        "  print (f'Model minimum count threshold={min_count}')\n",
        "  for item in model.wv.most_similar(query, topn=5):\n",
        "    print (item)\n",
        "  print ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model minimum count threshold=10\n",
            "('throwing', 0.6423540115356445)\n",
            "('pulling', 0.6338802576065063)\n",
            "('locked', 0.6327558159828186)\n",
            "('ran', 0.6293809413909912)\n",
            "('turned', 0.6111283302307129)\n",
            "\n",
            "Model minimum count threshold=25\n",
            "('pulling', 0.6581501960754395)\n",
            "('thrown', 0.6287826895713806)\n",
            "('run', 0.6103108525276184)\n",
            "('pulled', 0.6065022349357605)\n",
            "('turned', 0.5958484411239624)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVOnPvKFHGUU"
      },
      "source": [
        "**Observations**: Increasing the minimum count makes the model train faster since there are fewer tokens. It also potentially reduces the variance in the embeddings overall, since the model is only learning representations for words with more occurrences to train on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gHObJPezP5P"
      },
      "source": [
        "### Extracting data structures from gensim model\n",
        "\n",
        "The key elements of a trained gensim embeddings model are\n",
        "\n",
        "- a matrix of word vectors\n",
        "- a dictionary mapping from words in the vocabulary to rows of the matrix (and vice-versa)\n",
        "\n",
        "We can pull those elements of the model out and work with them -- mathematically transform the embeddings matrix, subset the vocabulary, and so on. We'll need to do those things in Section III. To make that easier, let's write a function that converts the embeddings from gensim to numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcd3Tv9G7dZP"
      },
      "source": [
        "def w2v_to_numpy (model):\n",
        "  \"\"\" Convert the word2vec model (the embeddings) into numpy arrays.\n",
        "  Also create and return the mapping of words to the row numbers.\n",
        "\n",
        "  Parameters:\n",
        "  ===========\n",
        "  model (gensim.Word2Vec): a trained gensim model\n",
        "\n",
        "  Returns:\n",
        "  ========\n",
        "  embeddings (numpy.ndarray): Embeddings of each word\n",
        "  idx, iidx (tuple): idx is a dictionary mapping word to row number\n",
        "                     iidx is a dictionary mapping row number to word\n",
        "  \"\"\"\n",
        "  model.wv.init_sims()\n",
        "  embeddings = deepcopy (model.wv.vectors_norm)\n",
        "  idx = {w:i for i, w in enumerate (model.wv.index2word)}\n",
        "  iidx = {i:w for i, w in enumerate (model.wv.index2word)}\n",
        "  return embeddings, (idx, iidx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNSe2YJwfX0o"
      },
      "source": [
        "model = Word2Vec (corpus, min_count=10, window=10, seed=42, workers=1)\n",
        "embs, (idx, iidx) = w2v_to_numpy (model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6GIMu8ujwWZ"
      },
      "source": [
        "Cosine similarity is the _dot product_ of normed vectors. Knowing that, we can write a function to get nearest neighbors that reproduces gensim's `most_similar`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2oZSzmz8OUQ"
      },
      "source": [
        "def near_neighbors (embs, query, word2rownum, rownum2word, k=5):\n",
        "  \"\"\" Get the `k` nearest neighbors for a `query`\n",
        "\n",
        "  Parameters:\n",
        "  ===========\n",
        "  embs (numpy.ndarray): The embeddings.\n",
        "  query (str): Word whose nearest neighbors are being found\n",
        "  word2rownum (dict): Map word to row number in the embeddings array\n",
        "  rownum2word (dict): Map rownum from embeddings array to word\n",
        "  k (int, default=5): The number of nearest neighbors\n",
        "\n",
        "  Returns:\n",
        "  ========\n",
        "  neighbors (list): list of near neighbors;\n",
        "                    size of the list is k and each item is in the form\n",
        "                    of word and similarity.\n",
        "  \"\"\"\n",
        "\n",
        "  sims = np.dot (embs, embs[word2rownum[query]])\n",
        "  indices = np.argsort (-sims)\n",
        "  return [(rownum2word[index], sims[index]) for index in indices[1:k+1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMhQIs7j4ox"
      },
      "source": [
        "We can check against gensim's `most_similar` function to verify that it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMlJC_RljDw2",
        "outputId": "741efc1d-a97d-4327-c9bb-b570a0dcfe4a"
      },
      "source": [
        "for nbor, sim in near_neighbors (embs, query, idx, iidx, k=5):\n",
        "  print (nbor, sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chair 0.5899362\n",
            "chairwoman 0.54281104\n",
            "president 0.48572737\n",
            "chairman 0.4525596\n",
            "gentlemen 0.40156382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ9R7YJVpNzt",
        "outputId": "2ecb9d87-294c-4331-8801-9020f102e86d"
      },
      "source": [
        "for item in model.wv.most_similar(query, topn=5):\n",
        "  print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('chair', 0.5899361968040466)\n",
            "('chairwoman', 0.5428110361099243)\n",
            "('president', 0.48572731018066406)\n",
            "('chairman', 0.45255953073501587)\n",
            "('gentlemen', 0.40156376361846924)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYLSnsNkDJkb"
      },
      "source": [
        "## III. Comparing word2vec models\n",
        "\n",
        "**How to identify variation across groups or change over time?**\n",
        "\n",
        "Above, we produced a single vector representation for each word. **But what about differences in word meaning and usage across groups?** One way to investigate variations in meaning is to split up a corpus by some variable and fit separate models on each subset of the corpus.\n",
        "\n",
        "Here, we'll explore group variation by **political party** in the Congressional Record corpus, comparing separate word2vec models of Democratic and Republican speeches. Which words do Democrats and Republicans use differently?\n",
        "\n",
        "One other notable social science application is to explore how words change in meaning over time -- you can try that in Section IV below!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4YWtW6Gs4Pv"
      },
      "source": [
        "Rodriguez & Spirling (2021) curated a list of 10 political words to explore the semantic performance of word embeddings models according to human raters. We'll see how much those words from their list differ by party:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzDdjOh25NM2"
      },
      "source": [
        "politics_words = [\n",
        "                  'freedom', 'justice', 'equality', 'democracy', # political abstractions\n",
        "                  'abortion', 'immigration', 'welfare', 'taxes', # partisan political issues\n",
        "                  'democrat', 'republican' # political parties\n",
        "                 ] # from Rodriguez and Spirling 2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIhZ4ss_tkX4"
      },
      "source": [
        "For convenience, we've already split the corpus into two separate files by party. We'll load those the same way we did previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3yLYp_-b1i"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/sandeepsoni/comparing-word2vec-models/main/data/cr_dem_subset.txt'\n",
        "local_file = 'cr_dem_subset.txt'\n",
        "dem_corpus = stream_data(link_to_file=url, local_file=local_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfrooYhhnSrN"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/sandeepsoni/comparing-word2vec-models/main/data/cr_rep_subset.txt'\n",
        "local_file = 'cr_rep_subset.txt'\n",
        "rep_corpus = stream_data(link_to_file=url, local_file=local_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4wodkVptycV"
      },
      "source": [
        "Then we fit the two models and convert them to numpy matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzeKqzU1R_oK"
      },
      "source": [
        "dem_model = Word2Vec (dem_corpus, window=10, min_count=10, seed=42, workers=1)\n",
        "rep_model = Word2Vec (rep_corpus, window=10, min_count=10, seed=42, workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFhzdrojXb_V"
      },
      "source": [
        "dem_embs, (dem_idx, dem_iidx) = w2v_to_numpy (dem_model)\n",
        "rep_embs, (rep_idx, rep_iidx) = w2v_to_numpy (rep_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBiz8NEJuR3v"
      },
      "source": [
        "Picking just one of the politics words and examining its nearest neighbors in each model, we can see some similarities as well as some suggestive differences in usage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVBIJDdEpqdR",
        "outputId": "79ba3b58-5a6f-4c9a-a2f5-8071c3f62255"
      },
      "source": [
        "query = 'taxes'\n",
        "print (f'Near neighbors for {query} in the dem corpus')\n",
        "for item in near_neighbors (dem_embs, query, dem_idx, dem_iidx, k=10):\n",
        "  print (item)\n",
        "print ()\n",
        "print (f'Near neighbors for {query} in the rep corpus')\n",
        "for item in near_neighbors (rep_embs, query, rep_idx, rep_iidx, k=10):\n",
        "  print (item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Near neighbors for taxes in the dem corpus\n",
            "('income', 0.7788943)\n",
            "('wealthy', 0.7563566)\n",
            "('subsidies', 0.73802614)\n",
            "('billionaires', 0.7362864)\n",
            "('millionaires', 0.72916466)\n",
            "('revenues', 0.70216465)\n",
            "('revenue', 0.6959213)\n",
            "('tax', 0.6905236)\n",
            "('payroll', 0.68340343)\n",
            "('deduction', 0.6705793)\n",
            "\n",
            "Near neighbors for taxes in the rep corpus\n",
            "('tax', 0.7427663)\n",
            "('income', 0.70089805)\n",
            "('revenue', 0.6790836)\n",
            "('wage', 0.6310799)\n",
            "('wages', 0.61288047)\n",
            "('raising', 0.60696566)\n",
            "('rates', 0.59552443)\n",
            "('deductions', 0.58935404)\n",
            "('payroll', 0.58304083)\n",
            "('loopholes', 0.5764691)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmtIkAfvrnc7"
      },
      "source": [
        "It is interesting to see the appearance of terms such as _millionaires, billionaires, wealthy_ as near neighbors to _taxes_ in the democratic corpus. Indeed, one of the [long-standing talking points](https://www.npr.org/2021/09/16/1036853972/biden-democrats-tax-the-rich-reagan-trump-reconciliation) of democrats is to tax the rich in order to improve socioeconomic balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJs93ZAzu0Rw"
      },
      "source": [
        "### Alignment of the models\n",
        "\n",
        "What if we want to compare vectors _across_ models?\n",
        "\n",
        "If we try to compare directly, the result is nonsensical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4tbLtnwjFDC",
        "outputId": "74a9301b-aed5-4ad8-9ba0-45c7a48d3575"
      },
      "source": [
        "dem_embs[dem_idx['the']].dot(rep_embs[rep_idx['the']]) # This doesn't make sense, don't do this!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55391634"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHNmvNYr9kNi"
      },
      "source": [
        "We would expect that for a common term such as _the_, the embeddings should roughly be the same. However, we get a much smaller cosine similarity than we'd expect.\n",
        "\n",
        "With that baseline in mind, it doesn't make sense to compare more substantive words this way either:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lExgLTTygdD1",
        "outputId": "3b14e032-06d6-48ad-ae63-4eb510b6002c"
      },
      "source": [
        "dem_embs[dem_idx['taxes']].dot(rep_embs[rep_idx['taxes']]) # Again, don't do this!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49579582"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbIw-eWTkpVQ"
      },
      "source": [
        "This issue happens because the embeddings for each group are created separately, hence, their geometric positions in vector space are **not directly comparable**. To compare them, we first have to _align_ the embeddings.\n",
        "\n",
        "One way to align matrices is called [Procrustes alignment](https://en.wikipedia.org/wiki/Procrustes_analysis), which uses singular value decomposition; [Hamilton et al 2016a](https://arxiv.org/pdf/1605.09096.pdf) use this method to align their historical word embeddings. Note that to do this alignment, each matrix first needs to be subset to the common vocabulary and reindexed appropriately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnspRmuDknvl"
      },
      "source": [
        "def procrustes(A, B):\n",
        "    \"\"\"\n",
        "    Learn the best rotation matrix to align matrix B to A\n",
        "    https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
        "    \"\"\"\n",
        "    # U, _, Vt = np.linalg.svd(B.dot(A.T))\n",
        "    U, _, Vt = np.linalg.svd(B.T.dot(A))\n",
        "    return U.dot(Vt)\n",
        "\n",
        "def intersect_vocab (idx1, idx2):\n",
        "  \"\"\" Intersect the two vocabularies\n",
        "\n",
        "  Parameters:\n",
        "  ===========\n",
        "  idx1 (dict): the mapping for vocabulary in the first group\n",
        "  idx2 (dict): the mapping for vocabulary in the second group\n",
        "\n",
        "  Returns:\n",
        "  ========\n",
        "  common_idx, common_iidx (tuple): the common mapping for vocabulary in both groups\n",
        "  \"\"\"\n",
        "  common = idx1.keys() & idx2.keys()\n",
        "  common_vocab = [v for v in common]\n",
        "\n",
        "  common_idx, common_iidx = {v:i for i,v in enumerate (common_vocab)}, {i:v for i,v in enumerate (common_vocab)}\n",
        "  return common_vocab, (common_idx, common_iidx)\n",
        "\n",
        "def align_matrices (mat1, mat2, idx1, idx2):\n",
        "  \"\"\" Align the embedding matrices and their vocabularies.\n",
        "\n",
        "  Parameters:\n",
        "  ===========\n",
        "  mat1 (numpy.ndarray): embedding matrix for first group\n",
        "  mat2 (numpy.ndarray): embedding matrix for second group\n",
        "\n",
        "  index1 (dict): the mapping dictionary for first group\n",
        "  index2 (dict): the mapping dictionary for the second group\n",
        "\n",
        "  Returns:\n",
        "  ========\n",
        "  remapped_mat1 (numpy.ndarray): the aligned matrix for first group\n",
        "  remapped_mat2 (numpy.ndarray): the aligned matrix for second group\n",
        "  common_vocab (tuple): the mapping dictionaries for both the matrices\n",
        "  \"\"\"\n",
        "  common_vocab, (common_idx, common_iidx) = intersect_vocab (idx1, idx2)\n",
        "  row_nums1 = [idx1[v] for v in common_vocab]\n",
        "  row_nums2 = [idx2[v] for v in common_vocab]\n",
        "\n",
        "  #print (len(common_vocab), len (common_idx), len (common_iidx))\n",
        "  remapped_mat1 = mat1[row_nums1, :]\n",
        "  remapped_mat2 = mat2[row_nums2, :]\n",
        "  #print (mat1.shape, mat2.shape, remapped_mat1.shape, remapped_mat2.shape)\n",
        "  omega = procrustes (remapped_mat1, remapped_mat2)\n",
        "  #print (omega.shape)\n",
        "  # rotated_mat2 = np.dot (omega, remapped_mat2)\n",
        "  rotated_mat2 = np.dot (remapped_mat2, omega)\n",
        "\n",
        "  return remapped_mat1, rotated_mat2, (common_idx, common_iidx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY9DOmYlwIb7"
      },
      "source": [
        "Now we can align the embeddings. This subsets and reindexes the Democrat model (but doesn't change those vectors), and it realigns the vectors in the Republican model to match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyZ5ZfPlt7mc"
      },
      "source": [
        "dem_aligned_embs, rep_aligned_embs, (common_idx, common_iidx) = align_matrices (dem_embs, rep_embs, dem_idx, rep_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXrVf7CwknN"
      },
      "source": [
        "We can confirm that, within each model, we still get the same nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBaSQVucX6ga",
        "outputId": "8d9ef4ce-5c8f-4b8f-9392-984ddddb9d53"
      },
      "source": [
        "near_neighbors(dem_aligned_embs, 'taxes', common_idx, common_iidx, k=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('income', 0.76216817),\n",
              " ('wealthy', 0.75255597),\n",
              " ('billionaires', 0.7292168),\n",
              " ('subsidies', 0.72896576),\n",
              " ('millionaires', 0.7190103)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_HDdwBaHkE4",
        "outputId": "ba287294-a7a6-411a-edee-ddfea4784b67"
      },
      "source": [
        "near_neighbors(rep_aligned_embs, 'taxes', common_idx, common_iidx, k=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tax', 0.74077487),\n",
              " ('income', 0.6959666),\n",
              " ('revenue', 0.68224835),\n",
              " ('wage', 0.62697864),\n",
              " ('wages', 0.61312854)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jGDa3GwxWb"
      },
      "source": [
        "But now the cosine similarity for the same word _across_ the two models is meaningful as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-KYYnlyiBg2",
        "outputId": "ff60e50a-94e3-4461-aded-868177d2bfbb"
      },
      "source": [
        "dem_aligned_embs[common_idx['the']].dot(rep_aligned_embs[common_idx['the']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8964326"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkq5mSXMgVO8",
        "outputId": "7f519226-134d-4715-a353-b8eed3df53c2"
      },
      "source": [
        "dem_aligned_embs[common_idx['taxes']].dot(rep_aligned_embs[common_idx['taxes']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80409056"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0l5NDXIw9gS"
      },
      "source": [
        "### Differences between the two parties\n",
        "\n",
        "Based on these models, which of those 10 politics words are the most and least similar between political parties?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3taNxOmjRK",
        "outputId": "bcbcf1c9-6822-4d40-bfc8-62a28be5c006"
      },
      "source": [
        "political_words_sims = [(w, dem_aligned_embs[common_idx[w]].dot(rep_aligned_embs[common_idx[w]])) for w in politics_words]\n",
        "for w,score in sorted (political_words_sims, key=lambda x:x[1], reverse=True):\n",
        "  print (w, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "democracy 0.85693556\n",
            "freedom 0.8467763\n",
            "democrat 0.83415246\n",
            "taxes 0.8143488\n",
            "immigration 0.77701044\n",
            "justice 0.7588856\n",
            "abortion 0.73585945\n",
            "equality 0.7305735\n",
            "republican 0.7074235\n",
            "welfare 0.66618466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTXRyu7Z_tak"
      },
      "source": [
        "On face value, the similarity suggests that democrats and republicans differed on issues of welfare, abortion, equality, and immigration but had better agreement on the issue of freedom, for instance. This exploratory analysis should suggest a line of further inquiry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-YpYl7xE1N"
      },
      "source": [
        "What about the most and least distinct words between Democrats and Republicans across the entire vocabulary?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3SoR_WiXyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ede71a2-c34a-46aa-b9d8-57ea5f0b126b"
      },
      "source": [
        "sims = [(w, dem_aligned_embs[common_idx[w]].dot(rep_aligned_embs[common_idx[w]])) for w in common_idx.keys()]\n",
        "\n",
        "# most similar\n",
        "for w, sim in sorted (sims, key=lambda x:x[1], reverse=True)[0:10]:\n",
        "  print (w, sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "years 0.9484314\n",
            "have 0.9481046\n",
            "award 0.9478347\n",
            "they 0.94510657\n",
            "will 0.94416183\n",
            "colleague 0.94339293\n",
            "regiment 0.9431594\n",
            "named 0.9426217\n",
            "thank 0.9418623\n",
            "their 0.941315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ5pgUYCmKMG",
        "outputId": "70e18867-ca5a-47ea-a29a-9292d0f85124"
      },
      "source": [
        "# least similar\n",
        "for w, sim in sorted (sims, key=lambda x:x[1], reverse=False)[0:10]:\n",
        "  print (w, sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dial 0.04015053\n",
            "hugely 0.062467344\n",
            "amnesty 0.0944559\n",
            "bust 0.116602615\n",
            "secured 0.13149652\n",
            "dynamics 0.1319687\n",
            "stacked 0.13991457\n",
            "todayand 0.15825704\n",
            "wreck 0.15876536\n",
            "controlling 0.15935266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK08O9gokpbZ"
      },
      "source": [
        "Individually, these extreme words don't appear hugely informative -- potentially for reasons related to corpus size and model stability. But looking that the overall distribution of word similarities between the two corpora gives a rough sense for how similar to expect words to be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCF8wq5-ZxT"
      },
      "source": [
        "compare_df = pd.DataFrame({'word':common_idx.keys(), 'cosine_similarity':[s for _,s in sims]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yrYfTDB8_KEx",
        "outputId": "082fc39e-6b41-4abb-a694-7428b9280bc6"
      },
      "source": [
        "compare_df.cosine_similarity.hist(bins=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb2e4b25510>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iShyqJLuY9Xs"
      },
      "source": [
        "Comparing the embeddings directly isn't the only way to measure the difference in meaning. Another way is to compare the nearest neighbors for a given word. [Hamilton et al (2016b)](https://aclanthology.org/D16-1229.pdf) call this second measure a measure of \"cultural shift\" (as opposed to \"linguistic drift\") and define it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdvl0x3btTnx"
      },
      "source": [
        "def near_neighbor_sim (embs1, embs2, query, index, k=10):\n",
        "  idx, iidx = index\n",
        "  nn1 = near_neighbors (embs1, query, idx, iidx, k=k)\n",
        "  nn2 = near_neighbors (embs2, query, idx, iidx, k=k)\n",
        "\n",
        "  common = {w for w, _ in nn1} | {w for w,_ in nn2}\n",
        "  scores1 = np.array([embs1[idx[w]].dot (embs1[idx[query]]) for w in common])\n",
        "  scores2 = np.array([embs2[idx[w]].dot (embs2[idx[query]]) for w in common])\n",
        "\n",
        "  return np.dot (scores1, scores2)/(np.linalg.norm (scores1) * np.linalg.norm (scores2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUKLN-syvMJ3",
        "outputId": "dceda8dc-2d0a-4807-8c07-2b7d2b552310"
      },
      "source": [
        "political_words_sims = [(w, near_neighbor_sim (dem_aligned_embs, rep_aligned_embs, w, (common_idx, common_iidx),k=10)) for w in politics_words]\n",
        "for w,score in sorted (political_words_sims, key=lambda x:x[1], reverse=True):\n",
        "  print (w, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "democracy 0.99120164\n",
            "freedom 0.98174524\n",
            "taxes 0.9791561\n",
            "equality 0.96484727\n",
            "democrat 0.96445477\n",
            "justice 0.9580842\n",
            "abortion 0.9032321\n",
            "republican 0.87475365\n",
            "welfare 0.8567218\n",
            "immigration 0.8076502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4GL2rswNRQ"
      },
      "source": [
        "We might also use randomization to check whether differences in meaning are statistically significant. Here's one idea for how to do that, based on reshuffling the documents in the corpus into random groups. The following procedure repeatedly and randomly exchanges the speeches between the two groups, training separate models on the randomly split data. For a given word, then, change metrics could be calculated across all models.\n",
        "\n",
        "More sophisticated approaches to randomization exist. For instance, some papers (Kozlowski et al 2019, Nelson 2021) use bootstrapping - resampling *with* replacement - to produce confidence intervals for estimates derived from word2vec models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvUBKwkbwvs5"
      },
      "source": [
        "def randomize_and_retrain (corpus1, corpus2, num_repetitions=3):\n",
        "  sents1 = [sent for sent in corpus1]\n",
        "  n_sents1 = len (sents1)\n",
        "  sents2 = [sent for sent in corpus2]\n",
        "  n_sents2 = len (sents2)\n",
        "\n",
        "  sents = sents1 + sents2\n",
        "\n",
        "  all_embs1 = list ()\n",
        "  all_embs2 = list ()\n",
        "  all_indices = list ()\n",
        "  for i in range (num_repetitions):\n",
        "    sample = random.sample (sents, n_sents1+n_sents2)\n",
        "    part1 = sample[0:n_sents1]\n",
        "    part2 = sample[n_sents1:n_sents1+n_sents2]\n",
        "\n",
        "    m1 = Word2Vec (part1, window=10, min_count=10, seed=42, workers=1)\n",
        "    m2 = Word2Vec (part2, window=10, min_count=10, seed=42, workers=1)\n",
        "\n",
        "    embs1, (idx1, iidx1) = w2v_to_numpy (m1)\n",
        "    embs2, (idx2, iidx2) = w2v_to_numpy (m2)\n",
        "\n",
        "    embs1, embs2, (idx, iidx) = align_matrices (embs1, embs2, idx1, idx2)\n",
        "    all_embs1.append (embs1)\n",
        "    all_embs2.append (embs2)\n",
        "    all_indices.append ((idx, iidx))\n",
        "\n",
        "  return all_embs1, all_embs2, all_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I0oUWL9xUo4"
      },
      "source": [
        "all_dem_embs1, all_rep_embs1, all_indices = randomize_and_retrain (dem_corpus, rep_corpus, num_repetitions=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ219NqS1rqk"
      },
      "source": [
        "This is a computationally expensive procedure because each time we have to retrain and re-align the models. To have enough placebo models to calculate p-values, we might need to do up to $100$ repetitions.\n",
        "\n",
        "Here's the cosine similarity metric for one of the most distinct words in the actual data, recalculated on the randomly split data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrSnrph25t8d",
        "outputId": "1f4508e1-3dc2-4d98-b26b-7cff04b29d7d"
      },
      "source": [
        "w = 'welfare'\n",
        "for i, _ in enumerate(all_indices):\n",
        "  e1 = all_dem_embs1[i]\n",
        "  e2 = all_rep_embs1[i]\n",
        "  ix = all_indices[i]\n",
        "  print(e1[ix[0][w]].dot(e2[ix[0][w]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.72181785\n",
            "0.7336098\n",
            "0.74528486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_x8-kJIHG8Z"
      },
      "source": [
        "## IV. Exploring substantive questions\n",
        "\n",
        "Now, use the rest of the metadata to divide up the CR corpus in different ways.\n",
        "\n",
        "**Try it out**: Explore diachronic semantic change, _i.e., what words have changed in meaning over time. You can do this by splitting the corpus by Congressional session, to see how Congressional language changes from 2009 (the 111th session) to 2017 (the 114th session).\n",
        "\n",
        "First, load the metadata for the sample of speeches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIEIUrgRe-di",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "09239a26-748a-4078-dcb4-9f5775c811a4"
      },
      "source": [
        "cr_meta = pd.read_csv('https://raw.githubusercontent.com/sandeepsoni/comparing-word2vec-models/main/data/cr_subset_metadata.csv')\n",
        "cr_meta.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>party</th>\n",
              "      <th>state</th>\n",
              "      <th>chamber</th>\n",
              "      <th>nonvoting</th>\n",
              "      <th>session_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F</td>\n",
              "      <td>D</td>\n",
              "      <td>NY</td>\n",
              "      <td>H</td>\n",
              "      <td>voting</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>D</td>\n",
              "      <td>VT</td>\n",
              "      <td>S</td>\n",
              "      <td>voting</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>D</td>\n",
              "      <td>OR</td>\n",
              "      <td>H</td>\n",
              "      <td>voting</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>D</td>\n",
              "      <td>CA</td>\n",
              "      <td>H</td>\n",
              "      <td>voting</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F</td>\n",
              "      <td>D</td>\n",
              "      <td>MO</td>\n",
              "      <td>S</td>\n",
              "      <td>voting</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  gender party state chamber nonvoting  session_id\n",
              "0      F     D    NY       H    voting         111\n",
              "1      M     D    VT       S    voting         111\n",
              "2      M     D    OR       H    voting         111\n",
              "3      M     D    CA       H    voting         111\n",
              "4      F     D    MO       S    voting         111"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6zURYnKCiCE"
      },
      "source": [
        "Then, join the speeches corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiigcbjWCpfG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akv4sWOnCqQ3"
      },
      "source": [
        "Split the corpus four ways and fit four separate models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkSAKTN1Cofe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5KkEHpCv_L"
      },
      "source": [
        "Extract the vectors and align each model to the first time period:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdublR6WDIki"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfGeBJmeDI_B"
      },
      "source": [
        "Explore the degree to which the 10 `politics_words` change over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p71Lg-iiDRHu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQjPMImVDRdo"
      },
      "source": [
        "**Try it out**: Test the following hypothesis:\n",
        "\n",
        "_Congressional speech around specific issues are becoming increasingly polarized over the years_.  \n",
        "\n",
        "This can be tested by training two models per session, aligning them, and then  measuring the difference in meaning for a set of words such as the political words that we have in this notebook. A measure of drift could be the average cosine distance between the words (or some other distance measure) which can serve as a proxy for polarization. The metric can be calculated for every session separately and aggregated to obtain a timeseries of polarization which could at least give some visual verification for the hypothesis.\n",
        "\n",
        "To test the hypothesis more rigorously, one could modify the randomization procedure to exchange speeches across time repeatedly to learn a distribution of timeseries under a null model. Then the observed values in the timeseries can be compared to this distribution to test for statistical significance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmMypd8FyrXm"
      },
      "source": [
        "Further resources:\n",
        "\n",
        "- Interested in other _word2vec_ style models: Read about [GloVe](https://nlp.stanford.edu/projects/glove/) and [fastText](https://fasttext.cc/), two popular alternatives.\n",
        "- A more thorough analysis of the sensitivity of parameters to the quality of the embeddings can be found [here](https://aclanthology.org/Q18-1008.pdf) and [here](https://arxiv.org/pdf/1804.09692.pdf)\n",
        "- One bottleneck to the general recipe described in this notebook is the costly alignment procedure. Read about some of the papers that do not have to do alignment [here](http://www.cs.cmu.edu/~nasmith/papers/bamman+dyer+smith.acl14.pdf), [here](https://arxiv.org/pdf/1906.01688.pdf), and [here](https://aclanthology.org/2020.acl-main.51.pdf).\n",
        "- A hot application area where comparing embeddings model is necessary is in tracking language changes over time. Read more about this topic [here](https://arxiv.org/pdf/1811.06278.pdf). Some of the methods described in this notebook form  the basis of papers such as [this](https://arxiv.org/pdf/1909.04189.pdf), [this](https://arxiv.org/pdf/2103.07538.pdf), [this](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf), and [this](https://journals.sagepub.com/eprint/BBJ88DCISJPBZUYPUAU4/full).\n",
        "- As much as word2vec, and the static embeddings they produce, useful and still actively researched, the more contemporary methods in NLP are based on contextual language models which map a word to multiple vectors instead of one vector dependent on the context in which the word appears in. You'll learn more about it in a subsequent tutorial but [here](https://arxiv.org/pdf/1902.06006.pdf) is a simple introduction to contextual embeddings. [Here](https://arxiv.org/pdf/2004.14118.pdf) is a paper based on contextual embeddings to identify changes in language.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BOqArj8NqhW"
      },
      "source": [
        "## Supplemental material: training the model on the full CR data set\n",
        "\n",
        "We won't cover this material during the tutorial for time and logistical constraints, but you can try it on your own later.\n",
        "\n",
        "We've hosted the full dataset for the _Congressional Record_ for the 111-114th Congresses on [Google Drive here](https://drive.google.com/drive/folders/1cbOUBJ53kEwhrXOISBQNwpEC50PC22WJ?usp=sharing). The main text file is approximately 400MB.\n",
        "\n",
        "Intuitively, more data ought to produce better representations of word meaning. To find out whether a larger data set produces a more meaningful word2vec model, you can copy the folder over to your own Google Drive (if you have the space for it!) and train a model on it.\n",
        "\n",
        "Examine the nearest neighbors for the keywords from the tutorial and compare the face validity to your previous models. (Rodriguez and Spirling 2021 propose a more formal human-based test for intrinsic validity. Extrinsic metrics like analogy tasks are common in the CS/NLP literature, but their usefulness for social-scientific questions isn't as clear.)\n",
        "\n",
        "To load a data set this large into Colab, you'll need to mount your Drive as follows. The model will take several minutes to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJknUdBPfCXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f88d99-b3e2-47e3-ca17-4cce173c8294"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwRzsy4QNCRv",
        "outputId": "d2d3d811-a5c9-4099-db82-c92bc9c06609"
      },
      "source": [
        "!ls /content/drive/My\\ Drive/word2vec_tutorial_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cr_111-114.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYvPpYzNTxe"
      },
      "source": [
        "full_corpus = LineSentence('/content/drive/My Drive/word2vec_tutorial_data/cr_111-114.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cnnqYNlNbp-"
      },
      "source": [
        "full_model = Word2Vec(full_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCKyoPjDQZVf",
        "outputId": "2c41cb72-a534-449d-d256-c4f007cd966b"
      },
      "source": [
        "full_model.total_train_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "591.7294504189995"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQnivlUxQcbI",
        "outputId": "e6fb3723-9cb5-4b60-e1e9-4948ef3663da"
      },
      "source": [
        "for word in politics_words:\n",
        "  print(word + \": \" + str(full_model.wv.most_similar(word, topn=5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freedom: [('liberty', 0.8520753979682922), ('freedoms', 0.7836979627609253), ('democracy', 0.6669175624847412), ('selfgovernment', 0.6083847284317017), ('blessings', 0.6081134676933289)]\n",
            "justice: [('antonin', 0.6525830030441284), ('justicethe', 0.642440676689148), ('justiceand', 0.6111997961997986), ('justices', 0.5589073896408081), ('justicethat', 0.5473117828369141)]\n",
            "equality: [('equal', 0.6429880261421204), ('liberty', 0.5993154048919678), ('freedom', 0.5991718769073486), ('racial', 0.5757337808609009), ('tolerance', 0.5734748840332031)]\n",
            "democracy: [('selfgovernment', 0.7812619209289551), ('liberty', 0.6720027923583984), ('freedom', 0.666917622089386), ('pluralistic', 0.6564258933067322), ('pluralism', 0.6238898038864136)]\n",
            "abortion: [('abortions', 0.8474483489990234), ('contraception', 0.7411530017852783), ('contraceptives', 0.7077350616455078), ('sterilization', 0.6809824705123901), ('sterilizations', 0.6785510182380676)]\n",
            "immigration: [('tort', 0.576901912689209), ('sentencing', 0.5254663228988647), ('tsca', 0.5001512765884399), ('legal', 0.46212679147720337), ('generalized', 0.4587135314941406)]\n",
            "welfare: [('nutrition', 0.49486756324768066), ('careby', 0.4524313509464264), ('maternal', 0.4435751438140869), ('antihunger', 0.43894147872924805), ('wellness', 0.4337405562400818)]\n",
            "taxes: [('taxesand', 0.7406411170959473), ('tax', 0.7109102606773376), ('revenues', 0.6459546685218811), ('premiums', 0.6404402256011963), ('wages', 0.6124645471572876)]\n",
            "democrat: [('republican', 0.7655088901519775), ('democratic', 0.7291401624679565), ('democratand', 0.6777266263961792), ('party', 0.6198328733444214), ('democrats', 0.614392876625061)]\n",
            "republican: [('democratic', 0.8775242567062378), ('democrat', 0.7655088901519775), ('majority', 0.6954680681228638), ('gop', 0.6741468906402588), ('democrats', 0.6162402629852295)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE70kb-_SVOw",
        "outputId": "cef194fd-c120-4ae0-ee86-1276f0ed826e"
      },
      "source": [
        "for word in politics_words:\n",
        "  print(word + \": \" + str(model.wv.most_similar(word, topn=5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freedom: [('liberty', 0.8142341375350952), ('conscience', 0.8119156360626221), ('liberties', 0.7927228212356567), ('religious', 0.7802432775497437), ('civil', 0.7660068273544312)]\n",
            "justice: [('criminal', 0.6601111888885498), ('scalia', 0.63294517993927), ('uphold', 0.6007159948348999), ('kagan', 0.5907998085021973), ('counsel', 0.5896000862121582)]\n",
            "equality: [('peace', 0.8181419968605042), ('solemn', 0.8179757595062256), ('beacon', 0.8173268437385559), ('sacred', 0.8134492039680481), ('enduring', 0.8034995794296265)]\n",
            "democracy: [('values', 0.7751235365867615), ('israel', 0.7256131172180176), ('republic', 0.7079468965530396), ('nation', 0.7061699628829956), ('allies', 0.7045486569404602)]\n",
            "abortion: [('parenthood', 0.7673788070678711), ('treatment', 0.7597946524620056), ('healthcare', 0.7593953013420105), ('planned', 0.7538892030715942), ('provider', 0.7461418509483337)]\n",
            "immigration: [('reform', 0.8119494915008545), ('broken', 0.801181435585022), ('comprehensive', 0.7987378835678101), ('customs', 0.7274340391159058), ('failure', 0.7268326282501221)]\n",
            "welfare: [('prevention', 0.8297046422958374), ('safety', 0.8225149512290955), ('outcomes', 0.8215827941894531), ('housing', 0.8214005827903748), ('equity', 0.8204036951065063)]\n",
            "taxes: [('income', 0.847271740436554), ('costs', 0.817642092704773), ('wages', 0.7949280738830566), ('dollars', 0.793447732925415), ('premiums', 0.7877381443977356)]\n",
            "democrat: [('democratic', 0.8282544016838074), ('whip', 0.7930744886398315), ('leader', 0.7924360036849976), ('party', 0.7695021629333496), ('republican', 0.7581955790519714)]\n",
            "republican: [('democratic', 0.9053901433944702), ('majority', 0.7722342014312744), ('democrat', 0.7581955790519714), ('party', 0.7166377305984497), ('tea', 0.7131907939910889)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D5HKBLaTkRS",
        "outputId": "00ea439b-a720-44a0-a147-b051bdc41dbb"
      },
      "source": [
        "# the smaller model was much faster to train!\n",
        "model.total_train_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.869292167999959"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_JmZ9NoXGra"
      },
      "source": [
        "dem_full_model = Word2Vec(LineSentence('/content/drive/My Drive/word2vec_tutorial_data/cr_dem.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj6jFphdY5iH"
      },
      "source": [
        "rep_full_model = Word2Vec(LineSentence('/content/drive/My Drive/word2vec_tutorial_data/cr_rep.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0thjzPvcNuF",
        "outputId": "518f246b-0051-4dac-c567-3f6c58e74d9c"
      },
      "source": [
        "for word in politics_words:\n",
        "  print(word + \": \" + str(dem_full_model.wv.most_similar(word, topn=5)))\n",
        "for word in politics_words:\n",
        "  print(word + \": \" + str(rep_full_model.wv.most_similar(word, topn=5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freedom: [('liberty', 0.7604895234107971), ('freedoms', 0.726006805896759), ('expression', 0.6652768850326538), ('equality', 0.6386710405349731), ('free', 0.611534595489502)]\n",
            "justice: [('labors', 0.5449870824813843), ('antonin', 0.5447324514389038), ('justices', 0.5230984091758728), ('transportations', 0.4961283802986145), ('defense', 0.46832096576690674)]\n",
            "equality: [('equal', 0.6622339487075806), ('liberty', 0.6565925478935242), ('freedom', 0.6386710405349731), ('tolerance', 0.5838267207145691), ('ideals', 0.5804051160812378)]\n",
            "democracy: [('liberty', 0.6540989875793457), ('citizenry', 0.6339601278305054), ('expression', 0.6326954364776611), ('society', 0.6189396381378174), ('freedoms', 0.6137824058532715)]\n",
            "abortion: [('abortions', 0.8089573383331299), ('contraception', 0.7032475471496582), ('contraceptives', 0.6047090291976929), ('contraceptive', 0.5925676226615906), ('parenthood', 0.5815621614456177)]\n",
            "immigration: [('tort', 0.561780571937561), ('tsca', 0.5492129921913147), ('judaism', 0.5267336368560791), ('penal', 0.5113988518714905), ('sentencing', 0.4990886151790619)]\n",
            "welfare: [('maternal', 0.5094422101974487), ('wellness', 0.5073250532150269), ('nutrition', 0.48850077390670776), ('wellbeing', 0.48100167512893677), ('determinants', 0.48076555132865906)]\n",
            "taxes: [('tax', 0.7046031355857849), ('revenues', 0.6494638919830322), ('takehome', 0.6488272547721863), ('salaries', 0.6331532001495361), ('royalties', 0.6292845606803894)]\n",
            "democrat: [('member', 0.6126307249069214), ('party', 0.6104139089584351), ('democratic', 0.5636293292045593), ('memberi', 0.5609453916549683), ('whip', 0.5450536012649536)]\n",
            "republican: [('gop', 0.7053953409194946), ('democratic', 0.655788779258728), ('majority', 0.62782222032547), ('republicans', 0.6097180843353271), ('tea', 0.5878502726554871)]\n",
            "freedom: [('liberty', 0.8718166351318359), ('freedoms', 0.7893763780593872), ('liberties', 0.6816065311431885), ('democracy', 0.6510787010192871), ('religion', 0.6428207755088806)]\n",
            "justice: [('antonin', 0.614728569984436), ('justiceand', 0.5987398028373718), ('justicethat', 0.5914344191551208), ('justices', 0.5901399254798889), ('defenseand', 0.5809598565101624)]\n",
            "equality: [('pluralism', 0.6358973979949951), ('liberty', 0.6325992345809937), ('selfgovernment', 0.6304875612258911), ('racial', 0.6158442497253418), ('tolerance', 0.6138352155685425)]\n",
            "democracy: [('selfgovernment', 0.7824631929397583), ('freedom', 0.6510787606239319), ('civilization', 0.6441974639892578), ('liberty', 0.6438285112380981), ('sovereignty', 0.6314087510108948)]\n",
            "abortion: [('abortions', 0.8655559420585632), ('lateterm', 0.7103245258331299), ('contraception', 0.6887962818145752), ('elective', 0.6884344816207886), ('contraceptives', 0.6603422164916992)]\n",
            "immigration: [('tort', 0.5357598066329956), ('amnesty', 0.5100518465042114), ('legal', 0.5082457065582275), ('trade', 0.4878956377506256), ('aliens', 0.4864453971385956)]\n",
            "welfare: [('snap', 0.5692825317382812), ('tanf', 0.516579270362854), ('tort', 0.5033520460128784), ('schip', 0.48564040660858154), ('meanstested', 0.4761262834072113)]\n",
            "taxes: [('tax', 0.726752758026123), ('taxesand', 0.688085675239563), ('costs', 0.627837598323822), ('premiums', 0.6247910261154175), ('wages', 0.6029013395309448)]\n",
            "democrat: [('republican', 0.8706971406936646), ('democratic', 0.7860960960388184), ('party', 0.6500309705734253), ('republicans', 0.5993639230728149), ('leaderships', 0.5881084203720093)]\n",
            "republican: [('democrat', 0.8706972599029541), ('democratic', 0.8086810111999512), ('republicans', 0.6293817162513733), ('majority', 0.6185510158538818), ('minority', 0.58555006980896)]\n"
          ]
        }
      ]
    }
  ]
}